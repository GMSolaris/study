1. Пользователь (разработчик) написал в канал поддержки, что у него уже 3 минуты происходит CRUD операция в MongoDB и её нужно прервать.

Вы как инженер поддержки решили произвести данную операцию:

напишите список операций, которые вы будете производить для остановки запроса пользователя
предложите вариант решения проблемы с долгими (зависающими) запросами в MongoDB

Для начала я бы посмотрел список операций и нашел его запрос через  db.currentOp()
Далее убил запрос через db.killOp(opid) 

Первым же очевидным узким местом для долгих запросов является индекс, нужно его либо построить либо перестроить.

2. Вы запустили инстанс Redis для использования совместно с сервисом, который использует механизм TTL. Причем отношение количества записанных key-value значений к количеству истёкших значений есть величина постоянная и увеличивается пропорционально количеству реплик сервиса.

При масштабировании сервиса до N реплик вы увидели, что:

сначала рост отношения записанных значений к истекшим
Redis блокирует операции записи
Как вы думаете, в чем может быть проблема?

Первым делом я начал бы смотреть на i\o диска и размер занятой ram.
Наверняка большое число реплик отъело доступный ram, редис начал скидывать данные на диск из за свопа,
в момент записи данных он блокирует обращение на запись в базе.

3. Вы подняли базу данных MySQL для использования в гис-системе. При росте количества записей, в таблицах базы, пользователи начали жаловаться на ошибки вида:

InterfaceError: (InterfaceError) 2013: Lost connection to MySQL server during query u'SELECT..... '

Первым делом смотрим в логи mysql и ищем нет ли там очевидных ошибок (в принципе любой поиск ошибок ведет в лог для начала)
Далее конечно смотрим на параметры connection_timeout и net_read_timeout если запрос выполняется дольше чем таймаут, то клиент получит лост.
Надо увеличивать таймаут, но обязательно и проверить запрос на корректность, использование и наличие индексов и тд.
Часто увеличивая таймаут мы просто множим проблему кривых запросов.
Вторым моментом будет определение размера max_allowed_packet, если сервер стал отдавать нам пакет больше чем может, надо увеличить это значение.

4. Вы решили перевести гис-систему из задачи 3 на PostgreSQL, так как прочитали в документации, что эта СУБД работает с большим объемом данных лучше, чем MySQL.

После запуска пользователи начали жаловаться, что СУБД время от времени становится недоступной. В dmesg вы видите, что:

postmaster invoked oom-killer

Как вы думаете, что происходит?

Как бы вы решили данную проблему?

Подобную задачу пришлось решать на практике не так давно в проде. После обновления до 14 postgres периодически приходил oom-killer и убивал базу.
```
Feb 21 17:50:59 gefest kernel: [3693248.291680] oom-kill:constraint=CONSTRAINT_NONE,nodemask=(null),cpuset=/,mems_allowed=0-1,global_oom,task_memcg=/system.slice/system-postgresql.slice/postgresql@14-main.
service,task=postgres,pid=3135115,uid=108
Feb 21 17:50:59 gefest kernel: [3693248.291707] Out of memory: Killed process 3135115 (postgres) total-vm:277317144kB, anon-rss:234237244kB, file-rss:744kB, shmem-rss:35416kB, UID:108 pgtables:459540kB oom
_score_adj:0
Feb 21 17:50:59 gefest systemd[1]: postgresql@14-main.service: A process of this unit has been killed by the OOM killer.
```
Картинка выглядела так
![alt text](postres-zbx.png "oomkiller")

Соответственно база после этого, если настроен корректно демон, перезапускается и какое-то время недоступна.
Первым делом смотрим на размер буферов, если в конфиге некорректно выставлены значения, то база будет сжирать всю память, придет киллер и естественно убьет самого жирного, а это 99,9% сам постгрес.
Плюс постгрес любит иногда утекать, по этому надо обязательно ставить все апдейты и фиксы.
Таким образом был настроен мониторинг размера свободной памяти и выставлены алерты, чтобы узнать заранее что памяти мало, плюс подрезаны буферы по этому ману https://www.enterprisedb.com/postgres-tutorials/how-tune-postgresql-memory
Установлены последние фиксы, в системе добавлен sysctl -w vm.overcommit_memory=2
Проблема решена.